{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vincenzo-Miracula/TallerPratico/blob/main/Extracci%C3%B3ndeDatos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping"
      ],
      "metadata": {
        "id": "AzYCXa_2bnij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "---\n",
        "* BeautifulSoup 4 (bs4) es una libreria de Python utilizada para el web scraping y el análisis de páginas web HTML/XML.\n",
        "* Proporciona una forma sencilla de extraer datos específicos de páginas web, permitiendo a los desarrolladores navegar a través del marcado HTML, identificar elementos deseados y acceder a su contenido o atributos.\n",
        "* Es ampliamente utilizada para extraer información de sitios web.\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I_va-q_ehd7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests"
      ],
      "metadata": {
        "id": "sGJT6Vu2g55-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.53 Safari/537.36\"}\n",
        "url = ''\n",
        "response = requests.get(url, headers)\n",
        "soup = bs(response.content, 'html.parser')"
      ],
      "metadata": {
        "id": "sK3u3pVvg-Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "titulo = soup.find('', {'class': ''}).text.strip()\n",
        "contenidos = soup.find_all('', {'class':''})\n",
        "\n",
        "for contenido in contenidos:\n",
        "  contenido = contenido.text.strip()\n",
        "  contenido = contenido.replace('\\n', ' ')\n",
        "  contenido = contenido.replace('\\xa0', ' ')\n",
        "  data.append((titulo, contenido))"
      ],
      "metadata": {
        "id": "Lvysxjn2hB3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API"
      ],
      "metadata": {
        "id": "N0a4q7w0by2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Mastodon](https://mastodon.social/settings/applications)"
      ],
      "metadata": {
        "id": "AhlpyeRVbvfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Mastodon.py"
      ],
      "metadata": {
        "id": "fai4mgJ_cNQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mastodon import Mastodon\n",
        "import time\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "AmuZW9h1cSy1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Mastodon(access_token=\"houlqlYzAb153Pn6st0o71SZG9r6TqHxpTleFOXHtAA\", api_base_url=\"https://mastodon.social\")\n",
        "\n",
        "hashtag = 'Juventus'  # Sustituir por el hashtag deseado\n",
        "limit = 1000"
      ],
      "metadata": {
        "id": "MnV7mIK8cUkb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_toots = []\n",
        "\n",
        "while len(all_toots) < limit:\n",
        "    results = m.timeline_hashtag(hashtag, limit=limit)\n",
        "    if not results:\n",
        "        break\n",
        "    last_id = results[-1].id - 1\n",
        "    all_toots.extend(results)\n",
        "\n",
        "columns = ['Date', 'ID', 'User', 'Text', 'Lang', 'Replies', 'Follower']\n",
        "data = []\n",
        "\n",
        "for status in all_toots:\n",
        "    fecha = status['created_at']\n",
        "    userID = status['account']['id']\n",
        "    username = status['account']['display_name']\n",
        "    text = status['content']\n",
        "    lang = status['language']\n",
        "    rep = status['replies_count']\n",
        "    follower = status['account']['followers_count']\n",
        "\n",
        "    data.append({\n",
        "        'Date': fecha,\n",
        "        'ID': userID,\n",
        "        'User': username,\n",
        "        'Text': text,\n",
        "        'Lang': lang,\n",
        "        'Replies': rep,\n",
        "        'Follower': follower\n",
        "    })\n",
        "\n",
        "dfToot = pd.DataFrame(data, columns=columns)"
      ],
      "metadata": {
        "id": "MpQYf9uKcdFs"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dfToot)"
      ],
      "metadata": {
        "id": "p4m8ue4Zecv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfToot['Text']"
      ],
      "metadata": {
        "id": "BMOOy8bV9wRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_contenido(text):\n",
        "    contenido_limpio = re.sub(r'#|<span>|</span>|<p>|<\\/p>|<a.*?>|<\\/a>|class=\".*?\"', '', text)\n",
        "    return contenido_limpio"
      ],
      "metadata": {
        "id": "MAgvIrsicdi6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dfToot['Text'] = dfToot['Text'].astype(str)\n",
        "dfToot['Text'] = dfToot['Text'].apply(extraer_contenido)"
      ],
      "metadata": {
        "id": "yHrc8jehcqTZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [YouTube](https://console.cloud.google.com/apis/api/youtube.googleapis.com/)"
      ],
      "metadata": {
        "id": "RoDSrFo9b1dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-api-python-client"
      ],
      "metadata": {
        "id": "EB-L9a__cwlV"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import googleapiclient.discovery\n",
        "from google.oauth2 import service_account\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "TzMfLE8n9Thn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "DEVELOPER_KEY = \"AIzaSyBvPKkwaL0-ljRza0PZQN1i55C7ES7K_3c\"\n",
        "next_page_token = None\n",
        "comments = []\n",
        "\n",
        "youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
        "\n",
        "with tqdm(total=len(comments), unit= 'comentarios', unit_scale=True) as pbar:\n",
        "  while True:\n",
        "    request = youtube.commentThreads().list(\n",
        "      part=\"snippet\",\n",
        "      videoId=\"...\",\n",
        "      textFormat='plainText',\n",
        "      maxResults=100,\n",
        "      pageToken=next_page_token\n",
        "  )\n",
        "    response = request.execute()\n",
        "\n",
        "    for item in response['items']:\n",
        "      comment = item['snippet']['topLevelComment']['snippet']\n",
        "      comments.append([\n",
        "          comment['authorDisplayName'],\n",
        "          comment['publishedAt'],\n",
        "          comment['likeCount'],\n",
        "          comment['textDisplay']\n",
        "      ])\n",
        "\n",
        "    next_page_token = response.get('nextPageToken')\n",
        "    if not next_page_token:\n",
        "      break\n",
        "\n",
        "    pbar.update(len(comments))\n",
        "\n",
        "df_yt = pd.DataFrame(comments, columns=['author', 'published_at', 'like_count', 'text'])"
      ],
      "metadata": {
        "id": "pyErZ_Xx9bUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_yt"
      ],
      "metadata": {
        "id": "GNDrRJNy9p5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_author(text):\n",
        "    contenido_limpio = re.sub(r'@', '', text)\n",
        "    return contenido_limpio"
      ],
      "metadata": {
        "id": "0IY6LgM7BV02"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_yt['author'] = df_yt['author'].apply(extraer_author)"
      ],
      "metadata": {
        "id": "GU9w1QpPB6u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def texto_limpio(text):\n",
        "    contenido_limpio = re.sub(r'&gt|;', '', text)\n",
        "    contenido_limpio = re.sub(r'  ', ' ', contenido_limpio)\n",
        "    return contenido_limpio"
      ],
      "metadata": {
        "id": "PjIKLQEZCQ-a"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_yt['text'] = df_yt['text'].apply(texto_limpio)"
      ],
      "metadata": {
        "id": "s8dhnhFCCAsM"
      },
      "execution_count": 82,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n5qfFCG7hhTO",
        "RBThy_aXsEz9",
        "QOIuYZk5hk3Z",
        "Uq7-3eSehqPl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}